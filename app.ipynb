{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c6c72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import re\n",
    "import json\n",
    "from langchain_community.chat_models import ChatOllama \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import chardet\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from langchain.docstore.document import Document as LangDocument\n",
    "\n",
    "MASTER_JS = r\"C:\\users\\tuf\\documents\\cloudysys_nickfury\\dcmsln\\dcmsln_201812\\Master\\PageJS\\Query\"\n",
    "BUSINESSRULE_DIR = r\"C:\\Users\\TUF\\Documents\\cloudysys_nickfury\\dcmsln\\BusinessRule\\Commons\"\n",
    "VECTOR_STORE_PATH = \"./common/vectorstore\"\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277a8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG function\n",
    "def remove_simplified_chinese(text: str) -> str:\n",
    "    return re.sub(r\"[^\\x00-\\x7F\\u4e00-\\u9fff\\n\\r\\t\\w\\s.,:;!?(){}[\\]\\\"'@#$%^&*\\-+=\\\\/]\", \"\", text)\n",
    "\n",
    "def read_file_with_detected_encoding(file_path: str) -> str:\n",
    "    print(file_path)\n",
    "\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw = f.read()\n",
    "    return raw.decode('GB2312', errors='ignore')\n",
    "\n",
    "def extract_vb_functions(file_path: str) -> List[LangDocument]:\n",
    "    raw_code = read_file_with_detected_encoding(file_path)\n",
    "\n",
    "    # 移除簡體中文註解\n",
    "    cleaned_code = remove_simplified_chinese(raw_code)\n",
    "\n",
    "    pattern = r\"(Public\\s+(Sub|Function)|Private\\s+(Sub|Function)|Sub|Function)\\s+\\w+\\s*\\(.*?\\)[\\s\\S]+?End\\s+(Sub|Function)\"\n",
    "    matches = re.finditer(pattern, cleaned_code, re.MULTILINE | re.IGNORECASE)\n",
    "\n",
    "    docs = []\n",
    "    file_name = Path(file_path).name\n",
    "\n",
    "    for match in matches:\n",
    "        full_func = match.group(0).strip()\n",
    "\n",
    "        # 擷取函式名稱\n",
    "        func_name_match = re.search(r\"(Sub|Function)\\s+(\\w+)\", full_func)\n",
    "        func_name = func_name_match.group(2) if func_name_match else \"unknown\"\n",
    "\n",
    "        doc = LangDocument(\n",
    "            page_content=full_func,\n",
    "            metadata={\n",
    "                \"source\": file_name,\n",
    "                \"function\": func_name\n",
    "            }\n",
    "        )\n",
    "        docs.append(doc)\n",
    "\n",
    "    return docs\n",
    "def collect_all_vb_chunks(directory: str) -> List[LangDocument]:\n",
    "    all_docs = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".vb\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                chunks = extract_vb_functions(file_path)\n",
    "                all_docs.extend(chunks)\n",
    "    return all_docs\n",
    "\n",
    "def build_vector_store(docs: List[LangDocument]):\n",
    "    print(f\"[info] Building vectorstore with {len(docs)} code chunks...\")\n",
    "    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "    os.makedirs(VECTOR_STORE_PATH, exist_ok=True)\n",
    "    db.save_local(VECTOR_STORE_PATH)\n",
    "    print(f\"[success] Vectorstore saved to: {VECTOR_STORE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3613243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立RAG\n",
    "# all_chunks = collect_all_vb_chunks(SOURCE_DIR)\n",
    "# build_vector_store(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b428793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessageClassification(BaseModel):\n",
    "    line: int = Field(...)\n",
    "    original: str = Field(...)\n",
    "    issue: str = Field(...)\n",
    "    replacement: str = Field(...)\n",
    "    reason: str = Field(...)\n",
    "\n",
    "# Json 輸出格式解析器\n",
    "parser = JsonOutputParser()\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "# 使用本地 LLM 模型\n",
    "embed_model = OllamaEmbeddings(model=\"nomic-embed-text\") \n",
    "fst_llm = ChatOllama(model=\"llama3:8B\") #提出\n",
    "sec_llm = ChatOllama(model=\"mistral:7B\") #審查\n",
    "third_llm = ChatOllama(model=\"phi3:3.8B\") #檢查改動是否符合原始邏輯，若符合則發動投票\n",
    "\n",
    "fst_prompt = PromptTemplate.from_template(\n",
    "  \"\"\"\n",
    "你是一位 JavaScript 專家，負責將使用過時、不推薦的語法重構為現代瀏覽器相容的寫法。\n",
    "\n",
    "請分析以下 JavaScript 原始碼片段，並使用 JSON 格式輸出需要重構的部分與建議的新寫法：\n",
    "\n",
    "=== 原始碼 ===\n",
    "{code}\n",
    "===\n",
    "\n",
    "請回傳格式如下（如無需修改則回傳空陣列）：\n",
    "[\n",
    "  {{\n",
    "    \"line\": <問題行號>,\n",
    "    \"original\": \"<原始碼>\",\n",
    "    \"issue\": \"<問題說明>\",\n",
    "    \"replacement\": \"<改寫後的程式碼>\",\n",
    "    \"reason\": \"<為何這樣修改>\"\n",
    "  }}\n",
    "]\n",
    "  \"\"\"\n",
    ")\n",
    "sec_prompt = PromptTemplate.from_template(\"\"\"\n",
    "你是一位 JavaScript Linter 工具的專家。\n",
    "\n",
    "請根據下列 JSON 內容，審查其中每一項修改提案是否合理，並針對語法正確性、現代相容性與實務可行性進行審查。\n",
    "\n",
    "=== 修改提案 ===\n",
    "{proposal}\n",
    "===\n",
    "\n",
    "請回傳以下 JSON 陣列，每個元素為一條審查意見：\n",
    "[\n",
    "  {{\n",
    "    \"line\": <提案行號>,\n",
    "    \"vote\": \"approve\" | \"reject\",\n",
    "    \"comment\": \"<說明你的理由>\"\n",
    "  }}\n",
    "]\n",
    "\"\"\" \n",
    ")\n",
    "third_prompt = PromptTemplate.from_template(\"\"\"\n",
    "你是語意一致性檢查專家。\n",
    "\n",
    "請比較以下原始碼與改寫碼，確認是否保留了相同的邏輯與功能。\n",
    "\n",
    "=== 原始碼 ===\n",
    "{original}\n",
    "\n",
    "=== 改寫碼 ===\n",
    "{rewritten}\n",
    "\n",
    "請回傳以下格式：\n",
    "{{\n",
    "  \"consistency\": \"high\" | \"medium\" | \"low\",\n",
    "  \"comment\": \"<改寫是否符合原始語意？是否有任何遺漏？>\"\n",
    "}}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "summary_prompt = PromptTemplate.from_template(\"\"\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6363db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成.js\n",
    "def write_modified_js(original_path: str, original_code: str, approved_changes: List[dict]):\n",
    "    modified_code = original_code\n",
    "    for item in approved_changes:\n",
    "        modified_code = modified_code.replace(item[\"original\"], item[\"replacement\"])\n",
    "    \n",
    "    p = Path(original_path)\n",
    "    output_path = p.with_name(f\"{p.stem}_modified{p.suffix}\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(modified_code)\n",
    "\n",
    "# 審核json\n",
    "def extract_json(text: str) -> list:\n",
    "    try:\n",
    "        # 找第一個 \"[\" 與最後一個 \"]\"\n",
    "        start = text.find('[')\n",
    "        end = text.rfind(']') + 1\n",
    "\n",
    "        if start == -1 or end <= start:\n",
    "            print(\"找不到 JSON 陣列\")\n",
    "            return []\n",
    "\n",
    "        json_str = text[start:end]\n",
    "\n",
    "        # 將反引號改為正常雙引號（處理 LLM 混 Markdown 輸出）\n",
    "        json_str = json_str.replace(\"`\", \"\\\"\")\n",
    "\n",
    "        # 修復 JSON 常見錯誤：如 \"replacement\": \"\" (xxx)\n",
    "        json_str = re.sub(r'(\"replacement\"\\s*:\\s*)\"\"\\s*\\(.*?\\)', r'\\1\"\"', json_str)\n",
    "\n",
    "        # 嘗試載入\n",
    "        return json.loads(json_str)\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"\\nJSON 解析失敗：{e}\")\n",
    "        print(\"原始輸出：\", text)\n",
    "        return []\n",
    "\n",
    "# 呼叫LLM\n",
    "def call_llm(llm, prompt):\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content if hasattr(response, \"content\") else response\n",
    "\n",
    "# ====== Main LLM Logic ======\n",
    "def analyze_message_with_multi_llm(code: str):\n",
    "    # Agent 1：提案改寫\n",
    "    fst_input = fst_prompt.format(code=code)\n",
    "    fst_response = call_llm(fst_llm, fst_input)\n",
    "    proposals = extract_json(fst_response)\n",
    "\n",
    "    # 印出反饋\n",
    "    print(\"fst_output:\", proposals)\n",
    "\n",
    "    if not proposals:\n",
    "        return {\"status\": \"no-change\", \"original\": code}\n",
    "\n",
    "    approved_results = []\n",
    "\n",
    "    for p in proposals:\n",
    "        # Agent 2：語法審查\n",
    "        sec_input = sec_prompt.format(proposal=json.dumps([p], ensure_ascii=False))\n",
    "        sec_result = extract_json(call_llm(sec_llm, sec_input))\n",
    "        # 印出反饋\n",
    "        print(\"sec_result:\", sec_result)\n",
    "        \n",
    "\n",
    "        # Agent 3：語意一致性\n",
    "        third_input = third_prompt.format(\n",
    "            original=p[\"original\"],\n",
    "            rewritten=p[\"replacement\"]\n",
    "        )\n",
    "        third_result = extract_json(call_llm(third_llm, third_input))\n",
    "        # 印出反饋\n",
    "        print(\"third_result:\", third_result)\n",
    "\n",
    "        # 投票機制整合\n",
    "        vote = (\n",
    "            (sec_result and sec_result[0][\"vote\"] == \"approve\")\n",
    "            and (third_result and third_result[\"consistency\"] in [\"high\", \"medium\"])\n",
    "        )\n",
    "\n",
    "        if vote:\n",
    "            approved_results.append(p)\n",
    "\n",
    "    return {\"status\": \"done\", \"approved\": approved_results}\n",
    "\n",
    "# ====== Folder Runner ======\n",
    "\n",
    "def run_folder_review(folder_path: str):\n",
    "    js_files = list(Path(folder_path).rglob(\"*.js\"))\n",
    "    print(f\"共發現 {len(js_files)} 個 JS 檔案\")\n",
    "\n",
    "    for file_path in js_files:\n",
    "        print(f\"\\n 分析：{file_path}\")\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            code = f.read()\n",
    "\n",
    "        result = analyze_message_with_multi_llm(code)\n",
    "\n",
    "        if result[\"status\"] == \"done\":\n",
    "            write_modified_js(file_path, result[\"original\"], result[\"approved\"])\n",
    "        else:\n",
    "            print(f\"無需修改：{file_path}\")\n",
    "\n",
    "# 單一個js FOR測試\n",
    "def run_first_js_review(folder_path: str):\n",
    "    js_files = list(Path(folder_path).rglob(\"*.js\"))\n",
    "    if not js_files:\n",
    "        print(\"找不到任何 JS 檔案\")\n",
    "        return\n",
    "\n",
    "    first_file = js_files[0]\n",
    "    print(f\"正在分析第一個 JS 檔案：{first_file}\")\n",
    "\n",
    "    with open(first_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        code = f.read()\n",
    "\n",
    "    result = analyze_message_with_multi_llm(code)\n",
    "\n",
    "    if result[\"status\"] == \"done\":\n",
    "        write_modified_js(first_file, result[\"original\"], result[\"approved\"])\n",
    "    else:\n",
    "        print(f\"無需修改：{first_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8bc4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_first_js_review(MASTER_JS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
